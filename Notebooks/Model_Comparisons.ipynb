{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cecda8a4",
   "metadata": {},
   "source": [
    "# Model Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "685f07fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Custom Functions\n",
    "sys.path.append(os.path.abspath('../Notebooks/Utilities')) \n",
    "import cust_utilities as utils\n",
    "\n",
    "# Maths, Pandas etc\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sci\n",
    "\n",
    "# Plots\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "from matplotlib.backends.backend_pdf import PdfPages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "755b5586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the model evaluation run resuls and combine into a single df\n",
    "#\n",
    "\n",
    "data_folder_path = utils.get_folder_path('Model Comparisons')\n",
    "file_name_pattern = '**/models_results_df**.pkl'\n",
    "data_files = glob.glob(str(Path(data_folder_path) / file_name_pattern), recursive=True)\n",
    "\n",
    "df_list = []\n",
    "for next_file in data_files:\n",
    "    df = pd.read_pickle(next_file, compression='zip')\n",
    "    df_list.append(df)\n",
    "models_evaluations_df = pd.concat(df_list, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6811d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe shape: (15, 16)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>study</th>\n",
       "      <th>training_source_data_run</th>\n",
       "      <th>training_results_run</th>\n",
       "      <th>search_features_detail</th>\n",
       "      <th>search_features_selection</th>\n",
       "      <th>CV_search_time</th>\n",
       "      <th>CV_best_parameters</th>\n",
       "      <th>features_detail</th>\n",
       "      <th>features_selection</th>\n",
       "      <th>model_name</th>\n",
       "      <th>prediction_time</th>\n",
       "      <th>mcc</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>specificity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IOWA_Rest</td>\n",
       "      <td>1b_EEG_Features_Results_Run_20250801_full_run</td>\n",
       "      <td>(2_Feature_Selection_Training_Run_20250812_sea...</td>\n",
       "      <td>([region],)</td>\n",
       "      <td>([[cf, pw, bw]],)</td>\n",
       "      <td>7.889680</td>\n",
       "      <td>{'classifier__criterion': 'entropy', 'classifi...</td>\n",
       "      <td>region</td>\n",
       "      <td>[cf, pw, bw]</td>\n",
       "      <td>RandomForest_v1</td>\n",
       "      <td>0.053513</td>\n",
       "      <td>0.438086</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.703704</td>\n",
       "      <td>0.826087</td>\n",
       "      <td>0.272727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IOWA_Rest</td>\n",
       "      <td>1b_EEG_Features_Results_Run_20250801_full_run</td>\n",
       "      <td>(2_Feature_Selection_Training_Run_20250812_sea...</td>\n",
       "      <td>([region],)</td>\n",
       "      <td>([[cf, pw, bw]],)</td>\n",
       "      <td>4.764382</td>\n",
       "      <td>{'classifier__C': 10, 'classifier__class_weigh...</td>\n",
       "      <td>region</td>\n",
       "      <td>[cf, pw, bw]</td>\n",
       "      <td>LogisticRegression_v1</td>\n",
       "      <td>0.081201</td>\n",
       "      <td>0.234413</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.695652</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.363636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IOWA_Rest</td>\n",
       "      <td>1b_EEG_Features_Results_Run_20250801_full_run</td>\n",
       "      <td>(2_Feature_Selection_Training_Run_20250812_sea...</td>\n",
       "      <td>([region],)</td>\n",
       "      <td>([[cf, pw, bw]],)</td>\n",
       "      <td>4.492693</td>\n",
       "      <td>{'classifier__activation': 'tanh', 'classifier...</td>\n",
       "      <td>region</td>\n",
       "      <td>[cf, pw, bw]</td>\n",
       "      <td>MLPClassifier_v1</td>\n",
       "      <td>0.026388</td>\n",
       "      <td>0.323268</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.454545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IOWA_Rest</td>\n",
       "      <td>1b_EEG_Features_Results_Run_20250801_full_run</td>\n",
       "      <td>(2_Feature_Selection_Training_Run_20250812_sea...</td>\n",
       "      <td>([channel],)</td>\n",
       "      <td>([[cf, pw, bw]],)</td>\n",
       "      <td>9.475886</td>\n",
       "      <td>{'classifier__criterion': 'entropy', 'classifi...</td>\n",
       "      <td>channel</td>\n",
       "      <td>[cf, pw, bw]</td>\n",
       "      <td>RandomForest_v1</td>\n",
       "      <td>0.062812</td>\n",
       "      <td>0.569378</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IOWA_Rest</td>\n",
       "      <td>1b_EEG_Features_Results_Run_20250801_full_run</td>\n",
       "      <td>(2_Feature_Selection_Training_Run_20250812_sea...</td>\n",
       "      <td>([channel],)</td>\n",
       "      <td>([[cf, pw, bw]],)</td>\n",
       "      <td>68.521576</td>\n",
       "      <td>{'classifier__C': 1, 'classifier__class_weight...</td>\n",
       "      <td>channel</td>\n",
       "      <td>[cf, pw, bw]</td>\n",
       "      <td>LogisticRegression_v1</td>\n",
       "      <td>0.012745</td>\n",
       "      <td>0.295790</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.727273</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       study                       training_source_data_run  \\\n",
       "0  IOWA_Rest  1b_EEG_Features_Results_Run_20250801_full_run   \n",
       "1  IOWA_Rest  1b_EEG_Features_Results_Run_20250801_full_run   \n",
       "2  IOWA_Rest  1b_EEG_Features_Results_Run_20250801_full_run   \n",
       "3  IOWA_Rest  1b_EEG_Features_Results_Run_20250801_full_run   \n",
       "4  IOWA_Rest  1b_EEG_Features_Results_Run_20250801_full_run   \n",
       "\n",
       "                                training_results_run search_features_detail  \\\n",
       "0  (2_Feature_Selection_Training_Run_20250812_sea...            ([region],)   \n",
       "1  (2_Feature_Selection_Training_Run_20250812_sea...            ([region],)   \n",
       "2  (2_Feature_Selection_Training_Run_20250812_sea...            ([region],)   \n",
       "3  (2_Feature_Selection_Training_Run_20250812_sea...           ([channel],)   \n",
       "4  (2_Feature_Selection_Training_Run_20250812_sea...           ([channel],)   \n",
       "\n",
       "  search_features_selection  CV_search_time  \\\n",
       "0         ([[cf, pw, bw]],)        7.889680   \n",
       "1         ([[cf, pw, bw]],)        4.764382   \n",
       "2         ([[cf, pw, bw]],)        4.492693   \n",
       "3         ([[cf, pw, bw]],)        9.475886   \n",
       "4         ([[cf, pw, bw]],)       68.521576   \n",
       "\n",
       "                                  CV_best_parameters features_detail  \\\n",
       "0  {'classifier__criterion': 'entropy', 'classifi...          region   \n",
       "1  {'classifier__C': 10, 'classifier__class_weigh...          region   \n",
       "2  {'classifier__activation': 'tanh', 'classifier...          region   \n",
       "3  {'classifier__criterion': 'entropy', 'classifi...         channel   \n",
       "4  {'classifier__C': 1, 'classifier__class_weight...         channel   \n",
       "\n",
       "  features_selection             model_name  prediction_time       mcc  \\\n",
       "0       [cf, pw, bw]        RandomForest_v1         0.053513  0.438086   \n",
       "1       [cf, pw, bw]  LogisticRegression_v1         0.081201  0.234413   \n",
       "2       [cf, pw, bw]       MLPClassifier_v1         0.026388  0.323268   \n",
       "3       [cf, pw, bw]        RandomForest_v1         0.062812  0.569378   \n",
       "4       [cf, pw, bw]  LogisticRegression_v1         0.012745  0.295790   \n",
       "\n",
       "     recall  precision  f1_score  specificity  \n",
       "0  1.000000   0.703704  0.826087     0.272727  \n",
       "1  0.842105   0.695652  0.761905     0.363636  \n",
       "2  0.842105   0.727273  0.780488     0.454545  \n",
       "3  0.842105   0.842105  0.842105     0.727273  \n",
       "4  0.578947   0.785714  0.666667     0.727273  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Explore the structure of the combined dataframe\n",
    "print(\"Dataframe shape:\", models_evaluations_df.shape)\n",
    "display(models_evaluations_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0166a5b",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7042fe92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully exported to CSV: Model_Comparisons_DF_20250813_1345.csv\n",
      "Total rows exported: 15\n",
      "File location: /Users/stuartgow/GitHub/EEG_ML_Pipeline/Data/Model Comparisons/Model_Comparisons_DF_20250813_1345.csv\n"
     ]
    }
   ],
   "source": [
    "# Export The Entire DF to CSV\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "csv_filename = f\"Model_Comparisons_DF_{timestamp}.csv\"\n",
    "csv_filepath = os.path.join(data_folder_path, csv_filename)\n",
    "if os.path.exists(csv_filepath):\n",
    "    raise FileExistsError(f'File Exists: {csv_pilepath}')\n",
    "\n",
    "models_evaluations_df.to_csv(csv_filepath, index=False)\n",
    "\n",
    "print(f\"Successfully exported to CSV: {csv_filename}\")\n",
    "print(f\"Total rows exported: {len(models_evaluations_df)}\")\n",
    "print(f\"File location: {csv_filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7e275b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the combined dataframe to Excel\n",
    "#\n",
    "\n",
    "# Create filename with timestamp\n",
    "from datetime import datetime\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "excel_filename = f\"model_comparisons_{timestamp}.xlsx\"\n",
    "excel_filepath = Path(data_folder_path) / excel_filename\n",
    "\n",
    "try:\n",
    "    # Export to Excel with multiple sheets\n",
    "    with pd.ExcelWriter(excel_filepath, engine='openpyxl') as writer:\n",
    "        # Main data sheet\n",
    "        models_evaluations_df.to_excel(writer, sheet_name='All_Model_Results', index=False)\n",
    "        \n",
    "        # Summary statistics sheet (if we have the required columns)\n",
    "        if 'model_name' in models_evaluations_df.columns:\n",
    "            summary_stats = models_evaluations_df.groupby('model_name').agg({\n",
    "                col: ['count', 'mean', 'std', 'min', 'max'] \n",
    "                for col in models_evaluations_df.select_dtypes(include=[np.number]).columns\n",
    "            }).round(4)\n",
    "            \n",
    "            # Flatten column names\n",
    "            summary_stats.columns = ['_'.join(col).strip() for col in summary_stats.columns.values]\n",
    "            summary_stats.to_excel(writer, sheet_name='Summary_Statistics')\n",
    "            \n",
    "            # Model comparison sheet (key metrics only)\n",
    "            if all(col in models_evaluations_df.columns for col in ['mcc', 'recall', 'precision', 'f1_score']):\n",
    "                key_metrics = models_evaluations_df.groupby('model_name')[['mcc', 'recall', 'precision', 'f1_score']].agg(['mean', 'std']).round(4)\n",
    "                key_metrics.columns = ['_'.join(col).strip() for col in key_metrics.columns.values]\n",
    "                key_metrics.to_excel(writer, sheet_name='Key_Metrics_Comparison')\n",
    "    \n",
    "    print(f\"‚úÖ Successfully exported to: {excel_filepath}\")\n",
    "    print(f\"üìä Total rows exported: {len(models_evaluations_df)}\")\n",
    "    print(f\"üìÅ File location: {excel_filepath}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error exporting to Excel: {e}\")\n",
    "    print(\"Attempting basic export...\")\n",
    "    \n",
    "    # Fallback: simple export\n",
    "    try:\n",
    "        models_evaluations_df.to_excel(excel_filepath, index=False)\n",
    "        print(f\"‚úÖ Basic export successful: {excel_filepath}\")\n",
    "    except Exception as e2:\n",
    "        print(f\"‚ùå Basic export also failed: {e2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99ef7ab",
   "metadata": {},
   "source": [
    "## TBD - plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5f9477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bar plots for MCC and Recall by model type\n",
    "#\n",
    "\n",
    "# Check if required columns exist\n",
    "required_cols = ['mcc', 'recall', 'model_name']\n",
    "missing_cols = [col for col in required_cols if col not in models_evaluations_df.columns]\n",
    "\n",
    "if missing_cols:\n",
    "    print(f\"Missing columns: {missing_cols}\")\n",
    "    print(\"Available columns:\", models_evaluations_df.columns.tolist())\n",
    "else:\n",
    "    # Create figure with subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Group by model type and calculate mean values\n",
    "    grouped_stats = models_evaluations_df.groupby('model_name').agg({\n",
    "        'mcc': ['mean', 'std'],\n",
    "        'recall': ['mean', 'std']\n",
    "    }).round(4)\n",
    "    \n",
    "    # Flatten column names\n",
    "    grouped_stats.columns = ['_'.join(col).strip() for col in grouped_stats.columns.values]\n",
    "    \n",
    "    # Plot 1: MCC by model type\n",
    "    model_types = grouped_stats.index\n",
    "    mcc_means = grouped_stats['mcc_mean']\n",
    "    mcc_stds = grouped_stats['mcc_std']\n",
    "    \n",
    "    bars1 = ax1.bar(model_types, mcc_means, yerr=mcc_stds, capsize=5, alpha=0.7, color='skyblue')\n",
    "    ax1.set_title('MCC (Matthews Correlation Coefficient) by Model Type', fontsize=12, fontweight='bold')\n",
    "    ax1.set_xlabel('Model Type')\n",
    "    ax1.set_ylabel('MCC Score')\n",
    "    ax1.set_ylim([0, 1])\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, mean_val, std_val in zip(bars1, mcc_means, mcc_stds):\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{mean_val:.3f}¬±{std_val:.3f}',\n",
    "                ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    # Plot 2: Recall by model type\n",
    "    recall_means = grouped_stats['recall_mean']\n",
    "    recall_stds = grouped_stats['recall_std']\n",
    "    \n",
    "    bars2 = ax2.bar(model_types, recall_means, yerr=recall_stds, capsize=5, alpha=0.7, color='lightcoral')\n",
    "    ax2.set_title('Recall by Model Type', fontsize=12, fontweight='bold')\n",
    "    ax2.set_xlabel('Model Type')\n",
    "    ax2.set_ylabel('Recall Score')\n",
    "    ax2.set_ylim([0, 1])\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, mean_val, std_val in zip(bars2, recall_means, recall_stds):\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{mean_val:.3f}¬±{std_val:.3f}',\n",
    "                ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Display summary statistics table\n",
    "    print(\"\\nSummary Statistics by Model Type:\")\n",
    "    display(grouped_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633b9061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative visualization: Show MCC and Recall side by side for each model\n",
    "#\n",
    "\n",
    "if not missing_cols:\n",
    "    # Create a single plot with MCC and Recall side by side\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n",
    "    \n",
    "    # Get unique model types\n",
    "    model_types = models_evaluations_df['model_name'].unique()\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(model_types)))\n",
    "    \n",
    "    x_pos = 0\n",
    "    x_labels = []\n",
    "    x_positions = []\n",
    "    bar_width = 0.4\n",
    "    \n",
    "    for i, model_type in enumerate(model_types):\n",
    "        model_data = models_evaluations_df[models_evaluations_df['model_name'] == model_type]\n",
    "        mcc_values = model_data['mcc'].values\n",
    "        recall_values = model_data['recall'].values\n",
    "        \n",
    "        # Create x positions for this model's runs\n",
    "        x_range = np.arange(x_pos, x_pos + len(mcc_values))\n",
    "        \n",
    "        # Plot MCC and Recall bars side by side\n",
    "        mcc_bars = ax.bar(x_range - bar_width/2, mcc_values, bar_width, \n",
    "                         alpha=0.7, color=colors[i], label=f'{model_type} - MCC')\n",
    "        recall_bars = ax.bar(x_range + bar_width/2, recall_values, bar_width, \n",
    "                           alpha=0.7, color=colors[i], hatch='///', label=f'{model_type} - Recall')\n",
    "        \n",
    "        # Add mean lines for MCC and Recall\n",
    "        mean_mcc = mcc_values.mean()\n",
    "        mean_recall = recall_values.mean()\n",
    "        \n",
    "        # MCC mean line\n",
    "        ax.axhline(y=mean_mcc, xmin=(x_pos-0.5)/(len(models_evaluations_df)*2), \n",
    "                  xmax=(x_pos+len(mcc_values)-0.5)/(len(models_evaluations_df)*2), \n",
    "                  color=colors[i], linestyle='--', linewidth=2, alpha=0.6)\n",
    "        \n",
    "        # Recall mean line (slightly offset)\n",
    "        ax.axhline(y=mean_recall, xmin=(x_pos-0.5)/(len(models_evaluations_df)*2), \n",
    "                  xmax=(x_pos+len(recall_values)-0.5)/(len(models_evaluations_df)*2), \n",
    "                  color=colors[i], linestyle=':', linewidth=2, alpha=0.6)\n",
    "        \n",
    "        # Store positions for labels\n",
    "        x_labels.extend([f'{model_type}\\nRun {j+1}' for j in range(len(mcc_values))])\n",
    "        x_positions.extend(x_range)\n",
    "        x_pos += len(mcc_values) + 2  # Add gap between model types\n",
    "    \n",
    "    ax.set_title('MCC and Recall Scores by Model Type and Run', fontsize=16, fontweight='bold')\n",
    "    ax.set_ylabel('Score', fontsize=12)\n",
    "    ax.set_xlabel('Model Runs', fontsize=12)\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xticks(x_positions)\n",
    "    ax.set_xticklabels(x_labels, rotation=45, ha='right')\n",
    "    \n",
    "    # Add a text box with legend explanation\n",
    "    ax.text(0.02, 0.98, 'Solid bars: MCC\\nHatched bars: Recall\\nDashed lines: MCC means\\nDotted lines: Recall means', \n",
    "            transform=ax.transAxes, fontsize=10, verticalalignment='top',\n",
    "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eeg_ml_pipeline_v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

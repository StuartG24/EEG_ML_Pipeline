{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "358b2d91",
   "metadata": {},
   "source": [
    "# 2. Feature Selection & Cleaning Pipeline\n",
    "\n",
    "Scope:\n",
    "- Load an EEG features superset and prepare a features subset suitable for ML training or prediction\n",
    "- For an entire EEG study with multiple sibjects\n",
    "- Using Scikit-Learn transformers and pipeline for a repeatable process\n",
    "\n",
    "The Pipeline Stages:\n",
    "- TBD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb2b7c8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Dependencies\n",
    "\n",
    "General dependencies:\n",
    "- python = 3.11.13\n",
    "- numpy = 2.0.2\n",
    "- scipy = 1.15.3\n",
    "- pandas = 2.2.3\n",
    "- matplotlib = 3.10.3\n",
    "\n",
    "ML dependencies:\n",
    "- scikit-learn = 1.6.1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded61da6",
   "metadata": {},
   "source": [
    "# Imports & Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b8095ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import warnings\n",
    "\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "# Custom Functions\n",
    "sys.path.append(os.path.abspath('../Notebooks/Utilities')) \n",
    "import cust_utilities as utils\n",
    "\n",
    "# Maths, Pandas etc\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sci\n",
    "\n",
    "# Plots\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# ML Prep\n",
    "# import sklearn\n",
    "# sklearn.set_config(display='diagram')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder, OneHotEncoder\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# ML Train\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, RocCurveDisplay\n",
    "\n",
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff68cac",
   "metadata": {},
   "source": [
    "# Classes & Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a48147",
   "metadata": {},
   "source": [
    "## EEG Results Inspection Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a90432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick Summary of the Results\n",
    "# \n",
    " \n",
    "def results_summary(results_df):\n",
    "\n",
    "    # Shape and key subject features, channels\n",
    "    print(f'Features Shape: {results_df.shape}')\n",
    "    subject_info_plot(results_df)\n",
    "    channels_check(results_df)\n",
    "\n",
    "    # Aperiodic and periodic components distributions\n",
    "    aperiodic_components_plot(results_df)\n",
    "    periodic_components_plot(results_df)\n",
    "    fit_error_plot(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08e3d70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots for subject info\n",
    "def subject_info_plot(features_df):\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(16, 6))\n",
    "\n",
    "    # PD histogram\n",
    "    axes[0].hist(features_df['pd'].dropna(), bins=3, color='skyblue', edgecolor='black')\n",
    "    axes[0].set_title('PD')\n",
    "    axes[0].set_xlabel('PD')\n",
    "    axes[0].set_ylabel('Count')\n",
    "\n",
    "    # Gender histogram\n",
    "    axes[1].hist(features_df['gender'].map({'M': 0, 'F': 1}), bins=2, color='skyblue', edgecolor='black', rwidth=0.8)\n",
    "    axes[1].set_title('Gender (0=M, 1=F)')\n",
    "    axes[1].set_xlabel('Gender')\n",
    "    axes[1].set_ylabel('Count')\n",
    "    axes[1].set_xticks([0, 1])\n",
    "    axes[1].set_xticklabels(['M', 'F'])\n",
    "\n",
    "    # Age boxplot\n",
    "    axes[2].boxplot(features_df['age'].dropna())\n",
    "    axes[2].set_title('Age')\n",
    "    axes[2].set_ylabel('Age')\n",
    "    axes[2].set_xticks([1])\n",
    "    axes[2].set_xticklabels(['Subjects'])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66504fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of channels\n",
    "def channels_check(features_df, expected_channels = 63):\n",
    "\n",
    "    from collections import Counter\n",
    "\n",
    "     # Get the max number of channels all subjects\n",
    "    chn_cols = [col for col in features_df.columns if col.startswith('chn_')]\n",
    "    chn_numbers = set([col.split('_')[1] for col in chn_cols if col.split('_')[1].isdigit()])\n",
    "    num_channels = len(chn_numbers)\n",
    "\n",
    "    # Get the number of channels with periodic components\n",
    "    exponent_cols = [col for col in chn_cols if col.endswith('exponent')]\n",
    "    # num_exp_channels = len(exponent_cols)\n",
    "    summary = []\n",
    "    for idx, row in features_df.iterrows():\n",
    "        subject_id = row['subject_id']\n",
    "\n",
    "        # Channels With Peaks & difference with expected\n",
    "        exponents = row[exponent_cols].dropna().values\n",
    "        num_channels = len(exponents)\n",
    "        chann_diff = num_channels - expected_channels\n",
    "        if chann_diff != 0:\n",
    "            summary.append({\n",
    "                'subject_id': row['subject_id'],\n",
    "                'channels': num_channels,\n",
    "                'difference': chann_diff,\n",
    "                })\n",
    "            \n",
    "    subject_counts = Counter([entry['channels'] for entry in summary])\n",
    "    print(f'Subjects Channel Counts')\n",
    "    print(f'- Expected {expected_channels}: {len(features_df) - len(summary)}')\n",
    "    print(f'- Differences: {dict(subject_counts)}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d77339a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plot of aperiodic components\n",
    "def aperiodic_components_plot(features_df):\n",
    "\n",
    "    # Extract all columns for offset and exponent & stack\n",
    "    offset_cols = [col for col in features_df.columns if col.endswith('_offset')]\n",
    "    exponent_cols = [col for col in features_df.columns if col.endswith('_exponent')]\n",
    "    offset_values = features_df[offset_cols].values.flatten()\n",
    "    exponent_values = features_df[exponent_cols].values.flatten()\n",
    "\n",
    "    # Remove NaNs\n",
    "    offset_values = offset_values[~np.isnan(offset_values)]\n",
    "    exponent_values = exponent_values[~np.isnan(exponent_values)]\n",
    "\n",
    "    # Box plots\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 8))\n",
    "    fig.suptitle('Aperiodic Features Distribution Across Subjects')\n",
    "\n",
    "    axes[0].boxplot(offset_values)\n",
    "    # axes[0].set_ylabel('Offset Value')\n",
    "    axes[0].set_xticks([1])\n",
    "    axes[0].set_xticklabels(['Offset'], fontsize=12)\n",
    "\n",
    "    axes[1].boxplot(exponent_values)\n",
    "    # axes[1].set_ylabel('Exponent Value')\n",
    "    axes[1].set_xticks([1])\n",
    "    axes[1].set_xticklabels(['Exponent'])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a358e734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plot of periodic components\n",
    "def periodic_components_plot(features_df):\n",
    "    \n",
    "    # Extract all columns for cf, pw, bw & stack\n",
    "    cf_cols = [col for col in features_df.columns if '_cf_' in col]\n",
    "    pw_cols = [col for col in features_df.columns if '_pw_' in col]\n",
    "    bw_cols = [col for col in features_df.columns if '_bw_' in col]\n",
    "\n",
    "    cf_values = features_df[cf_cols].values.flatten()\n",
    "    cf_values = cf_values[~np.isnan(cf_values)]\n",
    "    pw_values = features_df[pw_cols].values.flatten()\n",
    "    pw_values = pw_values[~np.isnan(pw_values)]\n",
    "    bw_values = features_df[bw_cols].values.flatten()\n",
    "    bw_values = bw_values[~np.isnan(bw_values)]\n",
    "\n",
    "    # Bosplots\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 8))\n",
    "    fig.suptitle('Periodic Features Distribution Across Subjects', fontsize=16)\n",
    "\n",
    "    axes[0].boxplot(cf_values)\n",
    "    # axes[0].set_ylabel('CF Value')\n",
    "    axes[0].set_xticks([1])\n",
    "    axes[0].set_xticklabels(['CF'])\n",
    "\n",
    "    axes[1].boxplot(pw_values)\n",
    "    # axes[1].set_ylabel('PW Value')\n",
    "    axes[1].set_xticks([1])\n",
    "    axes[1].set_xticklabels(['PW'])\n",
    "\n",
    "    axes[2].boxplot(bw_values)\n",
    "    # axes[2].set_ylabel('BW Value')\n",
    "    axes[2].set_xticks([1])\n",
    "    axes[2].set_xticklabels(['BW'])\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d6e2939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plot of fit measures\n",
    "def fit_error_plot(features_df):\n",
    "\n",
    "    # Collect all r_squared and error columns & stack\n",
    "    r2_cols = [col for col in features_df.columns if col.endswith('r_squared')]\n",
    "    error_cols = [col for col in features_df.columns if col.endswith('error')]\n",
    "    r2_values = features_df[r2_cols].values.flatten()\n",
    "    r2_values = r2_values[~np.isnan(r2_values)]\n",
    "    error_values = features_df[error_cols].values.flatten()\n",
    "    error_values = error_values[~np.isnan(error_values)]\n",
    "\n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    fig.suptitle('Fit Measures Distributions Across Subjects')\n",
    "\n",
    "    axes[0].boxplot(r2_values)\n",
    "    # axes[0].set_title('r_squared')\n",
    "    # axes[0].set_ylabel('Value')\n",
    "    axes[0].set_xticks([1])\n",
    "    axes[0].set_xticklabels(['r_squared'])\n",
    "\n",
    "    axes[1].boxplot(error_values)\n",
    "    # axes[1].set_title('error')\n",
    "    # axes[1].set_ylabel('Value')\n",
    "    axes[1].set_xticks([1])\n",
    "    axes[1].set_xticklabels(['error'])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765b9c7d",
   "metadata": {},
   "source": [
    "# Establish Features Extraction Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb2b277",
   "metadata": {},
   "source": [
    "## Setup Parameters etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8af95d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML Features Pipeline Run Define & Setup\n",
    "#\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# Study Details\n",
    "study_name = 'IOWA_Rest'\n",
    "dataset_ref = 'ds004584-1.0.0'\n",
    "eeg_run_folder = 'EEG_Processing_ds004584-1.0.0_20250626_trial_3_preprocess'\n",
    "# study_name = 'UNM_Oddball'\n",
    "# dataset_ref = 'ds003490-1.1.0'\n",
    "# eeg_run_id = '20250618'\n",
    "\n",
    "# Run/Test Mode\n",
    "test_mode = False\n",
    "\n",
    "# Execution Parameters\n",
    "run_summary = 'full_pipeline'\n",
    "ml_params = {'models': 'none'\n",
    "            }\n",
    "# -----------------------------------------------------------------------\n",
    "\n",
    "# Get existing study details, if exists\n",
    "study_folder_path = utils.get_folder_path('Study_' + study_name)\n",
    "study_info_df = pd.read_pickle(study_folder_path + '/study_inf_df.pkl', compression='zip')\n",
    "study_subjects_df = pd.read_pickle(study_folder_path + '/study_subjects_df.pkl', compression='zip')\n",
    "\n",
    "# Get all folder paths from study_info_df\n",
    "eeg_processing_results_path = study_info_df.loc[0, 'eeg_processing_results_path']\n",
    "ml_training_results_path = study_info_df.loc[0, 'ml_training_results_path']\n",
    "\n",
    "# Get EEG results files\n",
    "eeg_results_run_path = os.path.join(eeg_processing_results_path, eeg_run_folder)\n",
    "if not os.path.isdir(eeg_results_run_path):\n",
    "    raise FileNotFoundError(f'Directory not found: {eeg_results_run_path}')\n",
    "eeg_results_superset_file_path = utils.get_file_path(eeg_results_run_path, 'eeg_results_superset_df.pkl')\n",
    "eeg_run_parameters_file_path = utils.get_file_path(eeg_results_run_path, 'eeg_run_params_df.pkl')\n",
    "\n",
    "# Establish a new ML Training Run\n",
    "current_date = datetime.now().strftime('%Y%m%d')\n",
    "ml_run_id = f'ML_Training_{dataset_ref}_{current_date}_{run_summary}'\n",
    "ml_training_run_path = utils.extend_folder_path(ml_training_results_path, ml_run_id, exists_ok=False)\n",
    "\n",
    "# Create run df and save\n",
    "ml_run_params_df = pd.DataFrame({\n",
    "    'ml_run_id': [ml_run_id],\n",
    "    'study_name': [study_name],\n",
    "    'dataset_ref': [dataset_ref],\n",
    "    'ml_params': [ml_params]\n",
    "})\n",
    "ml_run_params_df.to_pickle(ml_training_run_path + '/ml_run_params_df.pkl', compression='zip')\n",
    "\n",
    "# Set progress messages, testing\n",
    "if test_mode:\n",
    "    VERBOSE = True\n",
    "else:\n",
    "    VERBOSE = False\n",
    "\n",
    "del current_date, eeg_processing_results_path, eeg_run_folder, eeg_results_run_path, ml_training_results_path, run_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e744b8a",
   "metadata": {},
   "source": [
    "## EEG Preprocessing Results Superset Load & Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95281fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the EEG preprocessing results superset\n",
    "#\n",
    "\n",
    "# Get features superset created after EEG preprocessing\n",
    "results_superset_df = pd.read_pickle(eeg_results_superset_file_path, compression='zip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c41533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect Data Before Cleaning\n",
    "#\n",
    "\n",
    "results_summary(results_superset_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b0fd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Check ICA, Epoch rejection levels and flag or reject subjects?\n",
    "# TODO: change features_superset_df to results_superset_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99ba0c7",
   "metadata": {},
   "source": [
    "## Create Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5628f001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish a data cleaning pipeline\n",
    "#\n",
    "\n",
    "# Seperate X features and y target\n",
    "target_name = 'pd'\n",
    "feature_names = features_superset_df.columns[features_superset_df.columns != target_name]\n",
    "X = features_superset_df[feature_names].copy()\n",
    "y = features_superset_df[target_name].copy()\n",
    "\n",
    "# Data Split : Training & Test, 80:20. NB cross-validation will be perfoermed using Training\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Features/Columns to drop\n",
    "features_drop_subj = ['subject_id']\n",
    "# Channels to drop: r2, error and anything over max channels eg chn66\n",
    "max_channel = 62\n",
    "channels_columns_all = [col for col in features_superset_df.columns if 'chn_' in col]\n",
    "features_drop_r2 = [col for col in channels_columns_all if col.endswith('r_squared')]\n",
    "features_drop_error = [col for col in channels_columns_all if col.endswith('error')]\n",
    "features_drop_extra_channels = [col for col in channels_columns_all if int(col.split('_')[1]) > max_channel]\n",
    "features_drop_channels = features_drop_r2 + features_drop_error + features_drop_extra_channels\n",
    "channel_columns_remaining = [col for col in channels_columns_all if col not in features_drop_channels]\n",
    "\n",
    "# Numerical and categorical values to transform\n",
    "features_num = ['age']\n",
    "features_cat = ['gender']\n",
    "features_channels = channel_columns_remaining\n",
    "\n",
    "# Pipeline to impute and then scale the channels\n",
    "channel_pipeline = Pipeline([\n",
    "    #('impute', IterativeImputer(max_iter=10, random_state=42)),\n",
    "    ('impute', SimpleImputer(strategy='mean')),\n",
    "    ('scale_chns', RobustScaler())\n",
    "    ])\n",
    "\n",
    "# Define the transformations for the ColumnTransformer\n",
    "data_transformations = [\n",
    "    ('drop', 'drop', features_drop_subj + features_drop_channels),\n",
    "    ('encode_cat', OneHotEncoder(drop='first', handle_unknown='infrequent_if_exist'), features_cat),\n",
    "    ('scale_num', StandardScaler(), features_num),\n",
    "    ('prep_chans', channel_pipeline, features_channels)\n",
    "    #('impute', IterativeImputer(max_iter=10, random_state=42), features_channels),\n",
    "    # ('impute', SimpleImputer(strategy='mean'), features_channels),\n",
    "    # ('scale_chns', RobustScaler(), features_channels),  # RobustScaler as not anormal dist, plus large outliers\n",
    "    ]\n",
    "data_preprocessing = ColumnTransformer(\n",
    "    transformers = data_transformations,\n",
    "    remainder = 'passthrough'\n",
    "    )\n",
    "\n",
    "# Put into a pipeline, purely for saving and later use\n",
    "features_extraction_pl = Pipeline([\n",
    "    ('features_extraction', data_preprocessing)\n",
    "])\n",
    "\n",
    "# Fit all the transformations to the features\n",
    "# data_preprocessing.fit(X_train)\n",
    "features_extraction_pl.fit(X_train)\n",
    "\n",
    "\n",
    "# TODO: What is the ML training impact if r2, error not dropped?\n",
    "# TODO: Approach to handling missing channel data, eg when not all 10 periodic peaks are present: eg more complex such as MICE?\n",
    "# TODO: IterativeImputer / MICE is time consuming\n",
    "# TODO: RobustScaler for channels as not anormal dist, plus large outliers\n",
    "# TODO: still large outliers for offset, exponent, CF, PW, BW accross most channels ... resolve these?\n",
    "# TODO: Consider log of aboe to reduce the outliers?\n",
    "# TODO: Compare no scaling at all and model performance\n",
    "\n",
    "# TODO: Review below again\n",
    "# Drop all columns containing 'cf', 'bw', or 'pw'\n",
    "# TODO: This significantly reduces recall and AUC, false negatives and false positives are increased\n",
    "# TODO: Dropping all peiodic other than CF doesn't make much difference!!\n",
    "# Drop all columns containing 'offset' or 'exponent'\n",
    "# TODO: This reduces AUC and false positives are increased\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2266fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint - Pipeline\n",
    "\n",
    "# print(\"Feature Extraction Pipeline Steps:\")\n",
    "# for name, step in features_extraction_pl.named_steps.items():\n",
    "#     print(f\"- {name}: {step}\")\n",
    "\n",
    "print(\"\\nColumnTransformer Details:\")\n",
    "ct = features_extraction_pl.named_steps['features_extraction']\n",
    "for name, trans, cols in ct.transformers_:\n",
    "    print(f\"- Transformer: {name}\")\n",
    "    print(f\"    Columns: {cols}\")\n",
    "    print(f\"    Transformer object: {trans}\\n\")\n",
    "\n",
    "print(\"\\nAll Pipeline Parameters\")\n",
    "for param, value in features_extraction_pl.get_params().items():\n",
    "    print(f\"- {param}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9afbd95",
   "metadata": {},
   "source": [
    "# Transform Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f6fcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the transformations to training data\n",
    "\n",
    "X_train_transformed = features_extraction_pl.transform(X_train)\n",
    "X_test_transformed = features_extraction_pl.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b25f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint - Data\n",
    "\n",
    "# Before & After\n",
    "print(\"Features Extraction / Transformed Data\")\n",
    "print(f'- Original Features Superset: {features_superset_df.shape}')\n",
    "print(f'- Extracted. X_train: {X_train_transformed.shape} and y_train: {y_train.shape}')\n",
    "print(f'- Extracted. X_test: {X_test_transformed.shape} and y_test: {y_test.shape}')\n",
    "\n",
    "# Feature Names\n",
    "temp_names_X_train_df = pd.DataFrame(X_train_transformed, columns=data_preprocessing.get_feature_names_out())\n",
    "temp_names_X_train_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(\"\\nFeatures Extracted * 63 Channels * 10 peaks\")\n",
    "print(list(temp_names_X_train_df.columns[:2]))\n",
    "print(list(temp_names_X_train_df.columns[2:4]))\n",
    "print(list(temp_names_X_train_df.columns[4:7]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77945b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint - Data\n",
    "\n",
    "# Aperiodic and periodic components distributions\n",
    "aperiodic_components_plot(temp_names_X_train_df)\n",
    "periodic_components_plot(temp_names_X_train_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101a218a",
   "metadata": {},
   "source": [
    "# ML Model Training & Pipeline Create"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743640dc",
   "metadata": {},
   "source": [
    "## Functions - Classification Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b94cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Display The Model Fit Results\n",
    "\n",
    "def print_search_results(search, duration):\n",
    "    print('------- Search Results --------')\n",
    "    all_search_results = pd.DataFrame(search.cv_results_)\n",
    "    print(f\"Score: {search.best_score_:.4f}. Mean: {np.mean(all_search_results['mean_test_score']):.4f} and STD {np.std(all_search_results['mean_test_score']):.4f}\")\n",
    "    print(f'Search Took: {duration:.2f} seconds')\n",
    "    print(f\"Best Parameters: {search.best_params_}\")\n",
    "    top_n = 10\n",
    "    print(f\"Top {top_n} out of {len(all_search_results)} combinations:\")\n",
    "    display(all_search_results[['rank_test_score', 'mean_test_score', 'mean_fit_time', 'mean_score_time', 'params']].sort_values(by='rank_test_score').head(top_n))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe8e7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Present the Evaluation Metrics for a Classification Model\n",
    "\n",
    "def classification_metrics(for_Model, X_test, y_test, y_pred):\n",
    "    plt.style.use('default')\n",
    "\n",
    "    # Calculate Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "    # Print various metrics\n",
    "    print(f'Accuracy: {metrics.accuracy_score(y_true=y_test, y_pred=y_pred):.4f}')\n",
    "    print(f'Precision: {metrics.precision_score(y_true=y_test, y_pred=y_pred, pos_label=1):.4f}')\n",
    "    print(f'Recall: {metrics.recall_score(y_true=y_test, y_pred=y_pred, pos_label=1):.4f}')\n",
    "    print(f'F1 Score {metrics.f1_score(y_true=y_test, y_pred=y_pred, pos_label=1):.4f}')\n",
    "    print(f'Specificity: {tn / (tn + fp):.4f}')\n",
    "    print(f'Hamming Loss {metrics.hamming_loss(y_true=y_test, y_pred=y_pred):.4f}')\n",
    "\n",
    "\n",
    "    # Plot Confusion Matrix\n",
    "    class_labels = for_Model.classes_\n",
    "    fig, ax = plt.subplots(figsize=(12,4))\n",
    "    ax.set_title('Confusion Matrix')\n",
    "    ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels).plot(ax=ax)\n",
    "    plt.show\n",
    "\n",
    "    y_probabilities = for_Model.predict_proba(X_test)[:, 1]\n",
    "    roc_auc_score = metrics.roc_auc_score(y_true=y_test, y_score=y_probabilities)\n",
    "    print(f'ROC-AUC Score {roc_auc_score:.4f}')\n",
    "    gini_score = 2 * roc_auc_score - 1\n",
    "    print(f'Gini Index: {gini_score:.4f}')\n",
    "\n",
    "    # Plot the ROC curve\n",
    "    fig, ax = plt.subplots(figsize=(6,4))\n",
    "    ax.set_title('ROC Curve')\n",
    "    roc_display = RocCurveDisplay.from_estimator(for_Model, X_test, y_test, ax=ax, pos_label=1)\n",
    "    plt.show()\n",
    "\n",
    "    plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc88e7c",
   "metadata": {},
   "source": [
    "## Model Training - Random Forest & Transformed X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcd9b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish a Model Pipeline - Using Proessed Data\n",
    "#\n",
    "\n",
    "# Pipeline, params & grid search define\n",
    "model_pipeline = Pipeline([\n",
    "    ('classifier', RandomForestClassifier(random_state=42, n_jobs=-1, verbose=False))\n",
    "    ])\n",
    "\n",
    "grid_params = {\n",
    "    'classifier__criterion': ['gini', 'entropy', 'log_loss'],     # Default gini. Tree split evaluation function\n",
    "    'classifier__n_estimators': [150, 175],                       # Default 100. Number of trees\n",
    "    'classifier__max_depth': [2, 5, 10],                          # Default none\n",
    "    # 'randomforestclassifier__max_leaf_nodes': [5, 50],\n",
    "    # 'randomforestclassifier__min_samples_split': [2, 5],\n",
    "    # 'randomforestclassifier__class_weight': ['balanced']            # Gives more importance to minority classes ... ?? Improves recall at the expense of precision\n",
    "    } \n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    model_pipeline, grid_params, \n",
    "    cv=5,\n",
    "    scoring='precision'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c61d30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search run\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "grid_search.fit(X_train_transformed, y_train)\n",
    "duration = time.perf_counter() - start_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2db54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint - Pipeline\n",
    "\n",
    "# print(\"Feature Extraction Pipeline Steps:\")\n",
    "# for name, step in model_pipeline.named_steps.items():\n",
    "#     print(f\"- {name}: {step}\")\n",
    "\n",
    "print(\"\\nAll Pipeline Parameters\")\n",
    "for param, value in model_pipeline.get_params().items():\n",
    "    print(f\"- {param}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a777f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search results\n",
    "print_search_results(grid_search, duration)\n",
    "\n",
    "# Get the Best Model & Calculate Predicted Y and Evaluate\n",
    "model_randforest = grid_search.best_estimator_\n",
    "y_pred = model_randforest.predict(X_test_transformed)\n",
    "classification_metrics(model_randforest, X_test_transformed, y_test, y_pred)\n",
    "\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation score:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bbd912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances\n",
    "importances = model_randforest.named_steps['classifier'].feature_importances_\n",
    "\n",
    "# Map feature importances to transformed feature names\n",
    "transformed_feature_names = data_preprocessing.get_feature_names_out()\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': transformed_feature_names,\n",
    "    'Importance': importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "# Keep only the top 25 most important features\n",
    "importance_df = importance_df.head(25)\n",
    "\n",
    "# print(importance_df)\n",
    "\n",
    "# Plot the feature importances with names horizontally\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(importance_df['Feature'], importance_df['Importance'], color='skyblue')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Importances (Sorted)')\n",
    "plt.gca().invert_yaxis()  # Invert y-axis to have the most important feature at the top\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17337409",
   "metadata": {},
   "source": [
    "## Model Training - Random Forest & Pipeline Test on X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008dd428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish a Model Pipeline\n",
    "#\n",
    "\n",
    "# Pipeline, params & grid search define\n",
    "model_pipeline = Pipeline([\n",
    "    ('test', features_extraction_pl),\n",
    "    ('classifier', RandomForestClassifier(random_state=42, n_jobs=-1, verbose=False))\n",
    "    ])\n",
    "\n",
    "grid_params = {\n",
    "    'classifier__criterion': ['gini', 'entropy', 'log_loss'],     # Default gini. Tree split evaluation function\n",
    "    'classifier__n_estimators': [150, 175],                       # Default 100. Number of trees\n",
    "    'classifier__max_depth': [2, 5, 10],                          # Default none\n",
    "    # 'randomforestclassifier__max_leaf_nodes': [5, 50],\n",
    "    # 'randomforestclassifier__min_samples_split': [2, 5],\n",
    "    # 'randomforestclassifier__class_weight': ['balanced']            # Gives more importance to minority classes ... ?? Improves recall at the expense of precision\n",
    "    } \n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    model_pipeline, grid_params, \n",
    "    cv=5,\n",
    "    scoring='precision'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f53fbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search run\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "# grid_search.fit(X_train_transformed, y_train)\n",
    "grid_search.fit(X_train, y_train)\n",
    "duration = time.perf_counter() - start_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccf354e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint - Pipeline\n",
    "\n",
    "# print(\"Feature Extraction Pipeline Steps:\")\n",
    "# for name, step in model_pipeline.named_steps.items():\n",
    "#     print(f\"- {name}: {step}\")\n",
    "\n",
    "print(\"\\nAll Pipeline Parameters\")\n",
    "for param, value in model_pipeline.get_params().items():\n",
    "    print(f\"- {param}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1570dc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search results\n",
    "print_search_results(grid_search, duration)\n",
    "\n",
    "# Get the Best Model & Calculate Predicted Y and Evaluate\n",
    "model_randforest = grid_search.best_estimator_\n",
    "y_pred = model_randforest.predict(X_test)\n",
    "classification_metrics(model_randforest, X_test, y_test, y_pred)\n",
    "\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation score:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc2a7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances\n",
    "importances = model_randforest.named_steps['classifier'].feature_importances_\n",
    "\n",
    "# Map feature importances to transformed feature names\n",
    "transformed_feature_names = data_preprocessing.get_feature_names_out()\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': transformed_feature_names,\n",
    "    'Importance': importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "# Keep only the top 25 most important features\n",
    "importance_df = importance_df.head(25)\n",
    "\n",
    "# print(importance_df)\n",
    "\n",
    "# Plot the feature importances with names horizontally\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(importance_df['Feature'], importance_df['Importance'], color='skyblue')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Importances (Sorted)')\n",
    "plt.gca().invert_yaxis()  # Invert y-axis to have the most important feature at the top\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79671793",
   "metadata": {},
   "source": [
    "## Test - Inspect the Data Transformation Executed During the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45943c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access feature extraction output\n",
    "feature_extractor = model_randforest.named_steps['test']\n",
    "X_train_transformed_compare = feature_extractor.transform(X_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10690b7",
   "metadata": {},
   "source": [
    "# Test - Full Model Run ..... it should be near perfect!?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7eef74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_pred = model_randforest.predict(X)\n",
    "classification_metrics(model_randforest, X, y, test_y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dce5c9",
   "metadata": {},
   "source": [
    "# WIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3180d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete a full pipeline, inc model?\n",
    "# Create complete pipeline\n",
    "# complete_pipeline = Pipeline([\n",
    "#     ('preprocessor', preprocessor),\n",
    "#     ('classifier', RandomForestClassifier())\n",
    "# ])\n",
    "# complete_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6c3f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: save the pipeline / transformer\n",
    "# from joblib import dump, load\n",
    "\n",
    "# # Save the pipeline\n",
    "# dump(pipeline, 'pipeline.joblib', compress=3)\n",
    "\n",
    "# # Load the pipeline\n",
    "# loaded_pipeline = load('pipeline.joblib')\n",
    "\n",
    "# And meta data save\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0c8bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "\n",
    "# file_path = os.path.join(ml_training_run_path, 'test_model.pkl')\n",
    "# container = (model_randforest)\n",
    "# with open(file_path, 'wb') as file:\n",
    "#     pickle.dump(container, file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eeg_ml_pipeline_v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "095b6647",
   "metadata": {},
   "source": [
    "# EEG Machine Learning Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbf2539",
   "metadata": {},
   "source": [
    "## TO DO - To Review\n",
    "\n",
    "Projects:\n",
    "- Pickle: https://www.perplexity.ai/search/in-a-jupyter-notebook-i-have-c-0LbAAH9ITFGfcPYaWlrt6Q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9990c076",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "\n",
    "General dependencies:\n",
    "- python = 3.11.13\n",
    "- numpy = 2.0.2\n",
    "- scipy = 1.15.3\n",
    "- pandas = 2.2.3\n",
    "- matplotlib = 3.10.3\n",
    "\n",
    "ML dependencies:\n",
    "- scikit-learn = 1.6.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea841715",
   "metadata": {},
   "source": [
    "# Imports & Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e36d4273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import os\n",
    "import gc\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Plots\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# ML\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder, OneHotEncoder\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, RocCurveDisplay\n",
    "\n",
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b859d1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to establish relative paths for a given folder\n",
    "def get_folder_path(folder_name, data_folder='Data'):\n",
    "    project_root = os.path.dirname(os.getcwd())\n",
    "    folder_path = os.path.join(project_root, data_folder, folder_name)\n",
    "    if not os.path.isdir(folder_path):\n",
    "        raise FileNotFoundError(f'Directory not found: {folder_path}')  \n",
    "    return folder_path\n",
    "\n",
    "# Utility function to create a new folder path, if not exists\n",
    "def make_folder_path(folder_name, data_folder='Data', exists_ok=True):\n",
    "    project_root = os.path.dirname(os.getcwd())\n",
    "    folder_path = os.path.join(project_root, data_folder, folder_name)\n",
    "    if os.path.exists(folder_path):\n",
    "        if not exists_ok:\n",
    "            raise FileExistsError(f\"Directory already exists: {folder_path}\")\n",
    "    else:\n",
    "        os.makedirs(folder_path)\n",
    "    return folder_path\n",
    "\n",
    " # Utility function to extend an existing folder path with a subfolder\n",
    "def extend_folder_path(base_folder, subfolder, exists_ok=True):\n",
    "    if not os.path.isdir(base_folder):\n",
    "        raise FileNotFoundError(f'Parent directory not found: {base_folder}')\n",
    "    extended_path = os.path.join(base_folder, subfolder)\n",
    "    if os.path.exists(extended_path):\n",
    "        if not exists_ok:\n",
    "            raise FileExistsError(f\"Directory already exists: {extended_path}\")\n",
    "    else:\n",
    "        os.makedirs(extended_path)\n",
    "    return extended_path\n",
    "\n",
    "# Utility function to check for the existence of a file in a given folder\n",
    "def get_file_path(folder, file_name):\n",
    "    file_path = os.path.join(folder, file_name)\n",
    "    if not os.path.isfile(file_path):\n",
    "        raise FileNotFoundError(f'File not found: {file_path}')\n",
    "    return file_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e162bdf7",
   "metadata": {},
   "source": [
    "# Classes & Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb52595a",
   "metadata": {},
   "source": [
    "## Binary Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a041cc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Display The Model Fit Results\n",
    "\n",
    "def print_search_results(search, duration):\n",
    "    print('------- Search Results --------')\n",
    "    all_search_results = pd.DataFrame(search.cv_results_)\n",
    "    print(f\"Score: {search.best_score_:.4f}. Mean: {np.mean(all_search_results['mean_test_score']):.4f} and STD {np.std(all_search_results['mean_test_score']):.4f}\")\n",
    "    print(f'Search Took: {duration:.2f} seconds')\n",
    "    print(f\"Best Parameters: {search.best_params_}\")\n",
    "    top_n = 10\n",
    "    print(f\"Top {top_n} out of {len(all_search_results)} combinations:\")\n",
    "    display(all_search_results[['rank_test_score', 'mean_test_score', 'mean_fit_time', 'mean_score_time', 'params']].sort_values(by='rank_test_score').head(top_n))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "375e9fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Present the Evaluation Metrics for a Classification Model\n",
    "\n",
    "def classification_metrics(for_Model, X_test, y_test, y_pred):\n",
    "    plt.style.use('default')\n",
    "\n",
    "    # Calculate Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "    # Print various metrics\n",
    "    print(f'Accuracy: {metrics.accuracy_score(y_true=y_test, y_pred=y_pred):.4f}')\n",
    "    print(f'Precision: {metrics.precision_score(y_true=y_test, y_pred=y_pred, pos_label=1):.4f}')\n",
    "    print(f'Recall: {metrics.recall_score(y_true=y_test, y_pred=y_pred, pos_label=1):.4f}')\n",
    "    print(f'F1 Score {metrics.f1_score(y_true=y_test, y_pred=y_pred, pos_label=1):.4f}')\n",
    "    print(f'Specificity: {tn / (tn + fp):.4f}')\n",
    "    print(f'Hamming Loss {metrics.hamming_loss(y_true=y_test, y_pred=y_pred):.4f}')\n",
    "\n",
    "\n",
    "    # Plot Confusion Matrix\n",
    "    class_labels = for_Model.classes_\n",
    "    fig, ax = plt.subplots(figsize=(12,4))\n",
    "    ax.set_title('Confusion Matrix')\n",
    "    ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels).plot(ax=ax)\n",
    "    plt.show\n",
    "\n",
    "    y_probabilities = for_Model.predict_proba(X_test)[:, 1]\n",
    "    roc_auc_score = metrics.roc_auc_score(y_true=y_test, y_score=y_probabilities)\n",
    "    print(f'ROC-AUC Score {roc_auc_score:.4f}')\n",
    "    gini_score = 2 * roc_auc_score - 1\n",
    "    print(f'Gini Index: {gini_score:.4f}')\n",
    "\n",
    "    # Plot the ROC curve\n",
    "    fig, ax = plt.subplots(figsize=(6,4))\n",
    "    ax.set_title('ROC Curve')\n",
    "    roc_display = RocCurveDisplay.from_estimator(for_Model, X_test, y_test, ax=ax, pos_label=1)\n",
    "    plt.show()\n",
    "\n",
    "    plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb58cff",
   "metadata": {},
   "source": [
    "# Setup & Features Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b66f77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML Pipeline Run Define & Setup\n",
    "#\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# Study Details\n",
    "study_name = 'IOWA_Rest'\n",
    "dataset_ref = 'ds004584-1.0.0'\n",
    "eeg_run_id = '20250618'\n",
    "# study_name = 'UNM_Oddball'\n",
    "# dataset_ref = 'ds003490-1.1.0'\n",
    "\n",
    "# Run/Test Mode\n",
    "test_mode = False\n",
    "\n",
    "# Execution Parameters\n",
    "ml_params = {'models': 'none'\n",
    "            }\n",
    "# -----------------------------------------------------------------------\n",
    "\n",
    "# Get existing study details, if exists\n",
    "study_folder_path = get_folder_path('Study_' + study_name)\n",
    "study_info_df = pd.read_pickle(study_folder_path + '/study_inf_df.pkl', compression='zip')\n",
    "study_subjects_df = pd.read_pickle(study_folder_path + '/study_subjects_df.pkl', compression='zip')\n",
    "\n",
    "# Get all folder paths from study_info_df\n",
    "# dataset_path = study_info_df.loc[0, 'dataset_path']\n",
    "eeg_processing_results_path = study_info_df.loc[0, 'eeg_processing_results_path']\n",
    "ml_training_results_path = study_info_df.loc[0, 'ml_training_results_path']\n",
    "\n",
    "# Establish a new ML Training Run\n",
    "current_date = datetime.now().strftime('%Y%m%d')\n",
    "ml_run_id = f'ML_Training_{dataset_ref}_{current_date}'\n",
    "ml_training_run_path = extend_folder_path(ml_training_results_path, ml_run_id, exists_ok=False)\n",
    "\n",
    "# Create run df and save\n",
    "ml_run_params_df = pd.DataFrame({\n",
    "    'ml_run_id': [ml_run_id],\n",
    "    'study_name': [study_name],\n",
    "    'dataset_ref': [dataset_ref],\n",
    "    'ml_params': [ml_params]\n",
    "})\n",
    "ml_run_params_df.to_pickle(ml_training_run_path + '/ml_run_params_df.pkl', compression='zip')\n",
    "\n",
    "# Get the EEG features\n",
    "eeg_run_id = eeg_run_id = f'EEG_Processing_{dataset_ref}_{eeg_run_id}'\n",
    "eeg_processing_run_path = os.path.join(eeg_processing_results_path, eeg_run_id)\n",
    "eeg_results_features_superset_df = pd.read_pickle(eeg_processing_run_path + '/eeg_results_features_superset_df.pkl', compression='zip')\n",
    "\n",
    "# Set progress messages, testing\n",
    "if test_mode:\n",
    "    VERBOSE = True\n",
    "else:\n",
    "    VERBOSE = False\n",
    "\n",
    "del current_date, eeg_processing_run_path, eeg_processing_results_path, eeg_run_id, ml_training_results_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c6077f",
   "metadata": {},
   "source": [
    "# Features Inspection, Cleaning & Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fcbacf5",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'copy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m study_features_cleaned_df = \u001b[43meeg_processing_run_path\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m()\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Drop some channels\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# TODO: Should this be in the EEG pipeline so that uniform features are produced?\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Drop all columns with channel number greater than 63 .... ?? or delete row 63 as perhaps an error\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# TODO: This actually reduces the AUC!?\u001b[39;00m\n\u001b[32m      7\u001b[39m cols_to_drop = [col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m study_features_cleaned_df.columns \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mchn_\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m col \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mint\u001b[39m(col.split(\u001b[33m'\u001b[39m\u001b[33m_\u001b[39m\u001b[33m'\u001b[39m)[\u001b[32m1\u001b[39m]) > \u001b[32m63\u001b[39m]\n",
      "\u001b[31mAttributeError\u001b[39m: 'str' object has no attribute 'copy'"
     ]
    }
   ],
   "source": [
    "study_features_cleaned_df = eeg_processing_run_path.copy()\n",
    "\n",
    "# Drop some channels\n",
    "# TODO: Should this be in the EEG pipeline so that uniform features are produced?\n",
    "# Drop all columns with channel number greater than 63 .... ?? or delete row 63 as perhaps an error\n",
    "# TODO: This actually reduces the AUC!?\n",
    "cols_to_drop = [col for col in study_features_cleaned_df.columns if 'chn_' in col and int(col.split('_')[1]) > 63]\n",
    "study_features_cleaned_df.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "# Drop all columns containing 'error' or 'r_squared'\n",
    "# TODO: This doesn't make much difference to the predictions\n",
    "cols_to_drop = [col for col in study_features_cleaned_df.columns if 'error' in col or 'r_squared' in col]\n",
    "study_features_cleaned_df.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "# Drop all columns containing 'cf', 'bw', or 'pw'\n",
    "# TODO: This significantly reduces recall and AUC, false negatives and false positives are increased\n",
    "# cols_to_drop = [col for col in study_features_cleaned_df.columns if any(x in col for x in ['cf', 'bw', 'pw'])]\n",
    "# study_features_cleaned_df.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "# TODO: Dropping all peiodic other than CF doesn't make much difference!!\n",
    "cols_to_drop = [col for col in study_features_cleaned_df.columns if any(x in col for x in ['bw', 'pw'])]\n",
    "study_features_cleaned_df.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "# Drop all columns containing 'offset' or 'exponent'\n",
    "# TODO: This reduces AUC and false positives are increased\n",
    "# cols_to_drop = [col for col in study_features_cleaned_df.columns if 'offset' in col or 'exponent' in col]\n",
    "# study_features_cleaned_df.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "# Drop features\n",
    "# # TODO: Increases false positives ....\n",
    "# dropped_variables = ['gender']\n",
    "# study_features_cleaned_df.drop(dropped_variables, axis=1, inplace=True)\n",
    "\n",
    "# # TODO: Increases false negatives, redices recall. ...... is this to be expected given the PD in age, look at control group ages?\n",
    "# dropped_variables = ['age']\n",
    "# study_features_cleaned_df.drop(dropped_variables, axis=1, inplace=True)\n",
    "\n",
    "# Drop features\n",
    "dropped_variables = ['subject_id']\n",
    "study_features_cleaned_df.drop(dropped_variables, axis=1, inplace=True)\n",
    "\n",
    "# Before and After\n",
    "print(study_features_raw_df.shape)\n",
    "print(study_features_cleaned_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8bd675",
   "metadata": {},
   "source": [
    "# Data Split ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb0c08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features (X) and target variable (y)\n",
    "targetName = \"pd\"\n",
    "featureNames = study_features_cleaned_df.columns[study_features_cleaned_df.columns != targetName]\n",
    "\n",
    "X = study_features_cleaned_df[featureNames]\n",
    "y = study_features_cleaned_df[targetName]\n",
    "\n",
    "# Split of training and testing data, 80:20\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "del targetName, featureNames, X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82a8a5d",
   "metadata": {},
   "source": [
    "# Transforms  ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a242ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish a transformation for categorical and numerical features\n",
    "numerical_features = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_features = X_train.select_dtypes(include=['object', 'bool']).columns\n",
    "\n",
    "print(f'Numerics {len(numerical_features)} \\n', numerical_features)\n",
    "print(f'Categoricals {len(categorical_features)} \\n', categorical_features)\n",
    "\n",
    "transformations = [\n",
    "    ('cat', OneHotEncoder(drop='first', handle_unknown='infrequent_if_exist'), categorical_features),\n",
    "    # ('num', RobustScaler(), numerical_features) - more false positives\n",
    "    # ('num', StandardScaler(), numerical_features) - AUC reduced\n",
    "    # ('num', MinMaxScaler(), numerical_features) -more false positives & AUC reduced\n",
    "    ('num', 'passthrough', numerical_features)\n",
    "]\n",
    "\n",
    "# Add to pipeline, and later add other actions such as dropping rows, imputing etc etc\n",
    "data_prep_pipeline = Pipeline([\n",
    "    #('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('col_transform', ColumnTransformer(transformers=transformations))\n",
    "])\n",
    "data_prep_pipeline.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88da6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the separate datasets\n",
    "X_train_transformed = data_prep_pipeline.transform(X_train)\n",
    "X_test_transformed = data_prep_pipeline.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30facfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(f'Original: {study_features_raw_df.shape}')\n",
    "display(f'Cleaned: {study_features_cleaned_df.shape}')\n",
    "\n",
    "display(f'X_Train: {X_train_transformed.shape}')\n",
    "display(f'X_Test: {X_test_transformed.shape}')\n",
    "display(data_prep_pipeline.get_feature_names_out())\n",
    "feature_names = data_prep_pipeline.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3197dbb",
   "metadata": {},
   "source": [
    "## Decision Tree - Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c6d430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Extend grid search to evaluate the model accuracy by dropping different columns, eg age, gender, bw, pw etc etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be9d619",
   "metadata": {},
   "source": [
    "See perplexity ideas for this: https://www.perplexity.ai/search/i-am-running-a-decision-tree-g-KnIOaRyfQpW.unFpG3rg.Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49412f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish a pipeline for the model and a grid search to get the best fitted model\n",
    "randforest_pipeline = make_pipeline(\n",
    "    RandomForestClassifier(random_state=42)\n",
    ")\n",
    "grid_params = {\n",
    "    'randomforestclassifier__criterion': ['gini', 'entropy', 'log_loss'],     # Default gini. Tree split evaluation function\n",
    "    'randomforestclassifier__n_estimators': [150, 175],                         # Default 100. Number of trees\n",
    "    'randomforestclassifier__max_depth': [2, 5, 10],                            # Default none\n",
    "    # 'randomforestclassifier__max_leaf_nodes': [5, 50],\n",
    "    # 'randomforestclassifier__min_samples_split': [2, 5],\n",
    "    # 'randomforestclassifier__class_weight': ['balanced']            # Gives more importance to minority classes ... ?? Improves recall at the expense of precision\n",
    "} \n",
    "\n",
    "# Run the search\n",
    "start_time = time.perf_counter()\n",
    "randforest_search = GridSearchCV(randforest_pipeline, \n",
    "                                 grid_params, \n",
    "                                 scoring='precision',\n",
    "                                 cv=5)\n",
    "randforest_search.fit(X_train_transformed, y_train)\n",
    "duration = time.perf_counter() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066583c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the results\n",
    "print_search_results(randforest_search, duration)\n",
    "\n",
    "# Get the Best Model & Calculate Predicted Y and Evaluate\n",
    "model_randforest = randforest_search.best_estimator_\n",
    "# model_randforest = rf_model\n",
    "y_pred = model_randforest.predict(X_test_transformed)\n",
    "classification_metrics(model_randforest, X_test_transformed, y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45eff761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances\n",
    "importances = model_randforest.named_steps['randomforestclassifier'].feature_importances_\n",
    "\n",
    "# Map feature importances to feature names\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "# Keep only the top 25 most important features\n",
    "importance_df = importance_df.head(25)\n",
    "\n",
    "# print(importance_df)\n",
    "\n",
    "# Plot the feature importances with names horizontally\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(importance_df['Feature'], importance_df['Importance'], color='skyblue')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Importances (Sorted)')\n",
    "plt.gca().invert_yaxis()  # Invert y-axis to have the most important feature at the top\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b098f057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "\n",
    "file_path = os.path.join(EEG_study_folder_path, 'test_model.pkl')\n",
    "container = (model_randforest)\n",
    "with open(file_path, 'wb') as file:\n",
    "    pickle.dump(container, file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057a1d36",
   "metadata": {},
   "source": [
    "# Working ......"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eeg_ml_pipeline_v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
